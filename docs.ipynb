{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6814a32",
   "metadata": {},
   "source": [
    "# For the Random Forest Algorithm\n",
    "    1) Number of trees(n_estimators):\n",
    "        after using gridsearch across multiple hyperparameters' values I found that the best number of trees is around 400.\n",
    "\n",
    "    2) Tree Depth (max_depth):\n",
    "        The most promising depth was around 4-7 and I chose 6 as it had a well-distributed recall, which is what I found most important \n",
    "        while using this model.\n",
    "\n",
    "    3) Class Weight (class_weight):\n",
    "        Using balanced as the class_weight as our data is biased towards the 'No' results with over 90% of the data being labeled 'No'.\n",
    "\n",
    "    4) Evaluation Metrics:\n",
    "        For the metrics used I focused on maintaing a relatively high accuracy while maintaining the 'Yes' recall as high as possible.\n",
    "\n",
    "    5) Hyperparameter Selection Criteria:\n",
    "        I used recall as the scoring metric during GridSearchCV to guide hyperparameter selection. This ensured the tuning process prioritized identifying patients likely to be readmitted within 30 days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea228938",
   "metadata": {},
   "source": [
    "# for data cleansing\n",
    "    1) importing the 2 sheets into pandas dataframes\n",
    "\n",
    "    2) displaying number of null values in the dataset by replacing the '?' with none values to be able to use the isnull function and display the number of null values in each coulmn\n",
    "\n",
    "    3) dropping any coulmn that has null values percentage more than or equal 90%\n",
    "\n",
    "    4) displaying the number of nulls in the remaining coulmns\n",
    "\n",
    "    5) put all the encoding into dictionaries to use in decoding the coulmns\n",
    "\n",
    "    6) decoded the coulmns using one hot encoding\n",
    "\n",
    "    7) putting age in better formate by removing the \"[]\" and \"()\" and then getting the average of the range in the cell\n",
    "\n",
    "    8) truning categorical data into numerical data to help in the training process in the models\n",
    "\n",
    "    9) removing any feature that has correlation less than 0.3 with the target feature\n",
    "\n",
    "    10) detecting and droping every outliers entrie's row by masking and detecting outlier rows one coulmn at a time and removing them by boolean indexing"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
